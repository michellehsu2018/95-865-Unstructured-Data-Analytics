{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgehc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/georgehc/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This demo draws heavily from the handwritten digit example in\n",
    "# Chapter 2 of Francois Chollet's \"Deep Learning with Python\" book.\n",
    "# I've added a simpler single-layer example first before moving to\n",
    "# the 2-layer example. -George Chen (CMU Fall 2017)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flattened_train_images = train_images.reshape(len(train_images), -1)  # flattens out each training image\n",
    "flattened_train_images = flattened_train_images.astype(np.float32) / 255  # rescale to be between 0 and 1\n",
    "flattened_test_images = test_images.reshape(len(test_images), -1)  # flattens out each test image\n",
    "flattened_test_images = flattened_test_images.astype(np.float32) / 255  # rescale to be between 0 and 1\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels_categorical = to_categorical(train_labels) #one-hot encoding\n",
    "test_labels_categorical = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_categorical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(flattened_train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0KJH0KJH0KJH0KJH0L9DYTiKcBcbGlWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f2cf630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this plots out one of the training images\n",
    "idx = np.random.randint(len(train_images))  # random training image index\n",
    "plt.imshow(train_images[idx], cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 9.5, 783.5, -0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADEAAAD8CAYAAADe6kx2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAxVJREFUeJztnSGy6kAQRSfAAlBIHAKHxKEwYIFi\nCVgEW8EgWAD+OTBsgMJROBwYWEBCnpviy9Cpeif971ExXTWH6TsTQhKSPM9D1an99QDKQBIUJEFB\nEhQkQaHx1wMIIYQkSeJpQ57nSdF6FzMhCQqSoCAJCpKgIAkKkqAgCQqSoCAJCpKgIAkKkqAgCQqS\noCAJCpKgIAkKkqAgCQqSoOBCAnFbxHa7NdUjJM7ns6k+IdxSqhtUgiQ4uJBArE79ft9Uj1idsiyL\ng6jX64VXJ8RMJEnhcf9bT5gJ7RNBEhxcSCBWpzRNTfWaibI4HA7xeDgcFq7XPkFBEhQQwT4ej6Z6\nhES73TbVIyQ+L9l8I4SQuN1upnrtExRcSCAy0Ww2TfWITFivdiAkrMFGtNN+vzfVI2ai1+vFQZxO\np/+znVwssS4kEMHOssxU72ImXAQb0U7WDxIhMZ1O4/Futytc76KdXATbhQQiE9Z9AiFxuVzicbfb\nLVzvItiImbD+yOJiJlysTi4kEJmYz+emekQmHo9HHESr1dJ37MqCyIT2ieCknVxIuMgEQmKz2cTj\nxWJRuF7BpoBoJ+0TwUk7uZBQJspCmQhOJBCZmEwmpnplggKinWazmake0U7v9zsOolar6ZJNZUFk\nYrlcmurVThRcSCAy8XlHwTcgMtFsNuMgXq+X9onKgsiEvp4GJ+3kQkKZKAtlIjiRQGTi+Xya6pUJ\nCi4kEJnQPhHUTuVhfVgQIeHiWqwyEZxIIDKhfSKoncrD+hCI2omCCwlEJnTaEZy0kwsJRCZcPAX8\n+ZbSb4RctJMLCUQ73e93U732CQqIdlqtVqZ6RDs1Go04iDRNq/kos96gEpxIINppPB6b6iVRFr1e\nz1SPWGL1Cr3gZHWSBAXE6rRer031iGDr70wCZCY6nU4cxPV61T5RWSRBAbE6DQYDUz0i2C7OYkej\nURzEz89PNSW0xAZJcJAEBUlQkAQFSVCQBAVJUJAEBUlQkAQFSVCQBAXEFUArLmZCEhQkQUESFCRB\nQRIUJEFBEhQkQUESFCRBQRIUfgFGi5nxKgekZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f2cf470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is just to visualize what the flattened version of the image above looks like;\n",
    "# after flattening the image I replicate it horizontally (by 10 pixels) just for\n",
    "# visualization purposes since otherwise the image is so thin (horizontally) that\n",
    "# it's not easy to see anything\n",
    "plt.imshow(np.hstack([train_images[idx].flatten().reshape((784,1))]*10), cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# extremely shallow single-layer model\n",
    "shallow_single_layer_model = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers\n",
    "shallow_single_layer_model.add(layers.Dense(10, activation='softmax', input_shape=(784,)))\n",
    "#only need to specify the input_shape for the first layer\n",
    "shallow_single_layer_model.summary()\n",
    "#Params=7,850 = 784*10(weight) + 10(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shallow_single_layer_model.compile(optimizer='rmsprop',\n",
    "                                   loss='categorical_crossentropy',\n",
    "                                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.6023 - acc: 0.8497\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3312 - acc: 0.9083\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.9161\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.2884 - acc: 0.9193\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2802 - acc: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181f2b5eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_single_layer_model.fit(flattened_train_images, #feature vector\n",
    "                               train_labels_categorical, #categorical version of labels\n",
    "                               epochs=5, #how many time to run through the training data\n",
    "                               batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test accuracy: 0.9223\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = shallow_single_layer_model.evaluate(flattened_test_images,\n",
    "                                                          test_labels_categorical)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# two-layer model\n",
    "two_layer_model = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers\n",
    "two_layer_model.add(layers.Dense(512, activation='relu', input_shape=(784,)))\n",
    "two_layer_model.add(layers.Dense(10, activation='softmax'))\n",
    "two_layer_model.compile(optimizer='rmsprop',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2534 - acc: 0.9275\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.1009 - acc: 0.9704\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0674 - acc: 0.9801\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0493 - acc: 0.9855\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0359 - acc: 0.9892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18309bda58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_layer_model.fit(flattened_train_images,\n",
    "                    train_labels_categorical,\n",
    "                    epochs=5,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 78us/step\n",
      "Test accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = two_layer_model.evaluate(flattened_test_images, test_labels_categorical)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
